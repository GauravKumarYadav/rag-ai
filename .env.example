# LLM Provider Configuration
# Options: lmstudio, ollama, openai, custom
LLM_PROVIDER=lmstudio

# LMStudio Settings (default provider)
# Start LMStudio, load a model, and enable the local server
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=Qwen3-VL-30B-Instruct

# Ollama Settings
# Install: https://ollama.ai, then run: ollama pull llama3
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# OpenAI Settings (requires API key)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Custom Endpoint (any OpenAI-compatible API)
CUSTOM_BASE_URL=http://localhost:8080/v1
CUSTOM_API_KEY=
CUSTOM_MODEL=default

# Common LLM Settings
LLM_TEMPERATURE=0.25
LLM_MAX_TOKENS=1024
LLM_TIMEOUT=60.0
CONTEXT_WINDOW=16000

# RAG Settings
CHROMA_DB_PATH=./data/chroma
EMBEDDING_MODEL=text-embedding-nomic-embed-text-v1.5

# Vector Store Configuration (Modular - can switch providers)
# Options: chromadb, pinecone, weaviate, qdrant, milvus
VECTOR_STORE_PROVIDER=chromadb

# For cloud providers (Pinecone, Weaviate Cloud, etc.)
# VECTOR_STORE_URL=https://your-instance.pinecone.io
# VECTOR_STORE_API_KEY=your-api-key
# VECTOR_STORE_NAMESPACE=your-index-name
# VECTOR_STORE_PREFIX=prod_  # Optional prefix for collection names

# Session Settings
SESSION_MAX_TOKENS=4000
SESSION_MAX_MESSAGES=20

# Server Settings
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
BACKEND_CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8080

