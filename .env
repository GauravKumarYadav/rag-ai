# LLM Provider Configuration
# Options: lmstudio, ollama, openai, custom
# lmstudio = Fast local inference (runs on host machine with GPU)
# ollama = Containerized inference (slower on CPU)
LLM_PROVIDER=ollama

# LMStudio Settings (RECOMMENDED for fast local inference)
# Start LMStudio, load a model, and enable the local server on port 1234
# For container access, we use host.containers.internal (Podman) or host.docker.internal (Docker)
LMSTUDIO_BASE_URL=http://host.containers.internal:1234/v1
LMSTUDIO_MODEL=local-model

# Ollama Settings (containerized - slower on CPU)
# Install: https://ollama.ai
# Models: ollama pull llama3.2:3b (chat), ollama pull nomic-embed-text (embeddings)
OLLAMA_BASE_URL=http://ollama:11434
OLLAMA_MODEL=mistral:latest

# OpenAI Settings (requires API key)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4o-mini

# Custom Endpoint (any OpenAI-compatible API)
CUSTOM_BASE_URL=http://localhost:8080/v1
CUSTOM_API_KEY=
CUSTOM_MODEL=default

# Common LLM Settings
LLM_TEMPERATURE=0.25
LLM_MAX_TOKENS=1024
LLM_TIMEOUT=180.0
CONTEXT_WINDOW=16000

# RAG Settings
CHROMA_DB_PATH=./data/chroma
EMBEDDING_MODEL=nomic-embed-text

# Embedding Service URL (microservices mode)
EMBEDDING_SERVICE_URL=http://embedding-service:8000

# Session Settings
SESSION_MAX_TOKENS=4000
SESSION_MAX_MESSAGES=20

# MySQL Settings (microservices mode)
MYSQL_HOST=mysql
MYSQL_PORT=3306
MYSQL_USER=root
MYSQL_PASSWORD=Sarita1!@2024_4
MYSQL_DATABASE=audit_logs

# Redis Settings
REDIS_URL=redis://redis:6379

# ChromaDB Settings
CHROMADB_HOST=chromadb
CHROMADB_PORT=8000

# Server Settings
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
BACKEND_CORS_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://localhost:8080,http://localhost,http://localhost:80
DOCLING_TELEMETRY_OPTOUT=1
CHAT_MODEL=mistral:latest
