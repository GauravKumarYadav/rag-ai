version: "3.9"

services:
  # ===========================================
  # INFRASTRUCTURE SERVICES
  # ===========================================
  
  # Redis - Sessions, Cache, Memory Persistence
  redis:
    image: redis:7-alpine
    container_name: rag-redis
    ports:
      - "6380:6379"   # host:container — use 6380 on host if 6379 is in use; Redis still listens on 6379 inside
    volumes:
      - rag-redis-data:/data
    command: redis-server --appendonly yes --maxmemory 1gb --maxmemory-policy allkeys-lru
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ChromaDB - Vector Database
  chromadb:
    image: chromadb/chroma:latest
    container_name: rag-chromadb
    ports:
      - "8020:8000"
    volumes:
      - ./data/chroma:/data
    environment:
      - IS_PERSISTENT=TRUE
      - ANONYMIZED_TELEMETRY=FALSE
      - ALLOW_RESET=TRUE
    restart: unless-stopped
    # Resource limits so one service does not starve the host
    mem_limit: 2g
    cpus: '1'
    # ChromaDB doesn't have curl, use wget or just skip health check
    healthcheck:
      test: ["CMD-SHELL", "python -c 'import urllib.request; urllib.request.urlopen(\"http://localhost:8000/api/v1/heartbeat\")' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s

  # ===========================================
  # APPLICATION SERVICES
  # ===========================================
  
  # Chat API - Main Backend with LangGraph Agents
  chat-api:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: rag-chat-api
    ports:
      - "8000:8000"
    env_file:
      - ./.env
    environment:
      # LLM Configuration (LM Studio)
      - LLM__PROVIDER=${LLM__PROVIDER:-groq}
      - LLM__LMSTUDIO__BASE_URL=${LLM__LMSTUDIO__BASE_URL:-http://host.docker.internal:1234/v1}
      - LLM__LMSTUDIO__MODEL=${LLM__LMSTUDIO__MODEL:-Qwen3-VL-30B-Instruct}
      - LLM__GROQ__BASE_URL=${LLM__GROQ__BASE_URL:-https://api.groq.com/openai/v1}
      - LLM__GROQ__MODEL=${LLM__GROQ__MODEL:-llama-3.3-70b-versatile}
      - LLM__GROQ__API_KEY=${LLM__GROQ__API_KEY}
      - LLM__TEMPERATURE=${LLM__TEMPERATURE:-0.35}
      - LLM__MAX_TOKENS=${LLM__MAX_TOKENS:-4096}
      - LLM__CONTEXT_WINDOW=${LLM__CONTEXT_WINDOW:-32000}
      # Embedding
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-auto}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-text-embedding-nomic-embed-text-v1.5}
      # RAG Configuration
      - RAG__PROVIDER=chromadb
      - RAG__URL=http://chromadb:8000
      - RAG__RERANKER_ENABLED=${RAG__RERANKER_ENABLED:-true}
      - RAG__BM25_ENABLED=${RAG__BM25_ENABLED:-true}
      - RAG__BM25_PERSIST_PATH=/app/data/bm25
      # Redis
      - REDIS__URL=redis://redis:6379
      - REDIS__SESSION_TTL=${REDIS__SESSION_TTL:-86400}
      # Memory Settings
      - MEMORY__MAX_CONTEXT_TOKENS=${MEMORY__MAX_CONTEXT_TOKENS:-4000}
      - MEMORY__SUMMARY_TARGET_TOKENS=${MEMORY__SUMMARY_TARGET_TOKENS:-1000}
      - MEMORY__SLIDING_WINDOW_SIZE=${MEMORY__SLIDING_WINDOW_SIZE:-10}
      # Agent Settings
      - AGENT__ENABLED=true
      - AGENT__MAX_STEPS=${AGENT__MAX_STEPS:-10}
      # JWT Authentication
      - JWT__SECRET_KEY=${JWT__SECRET_KEY:-change-me-in-production}
      - JWT__ALGORITHM=${JWT__ALGORITHM:-HS256}
      - JWT__ACCESS_TOKEN_EXPIRE_MINUTES=${JWT__ACCESS_TOKEN_EXPIRE_MINUTES:-60}
      # LangSmith Tracing
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2:-true}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY:-}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT:-agentic-rag}
      # Server
      - SERVER__HOST=0.0.0.0
      - SERVER__PORT=8000
    volumes:
      - ./data/bm25:/app/data/bm25
      - ./data/raw:/app/data/raw:ro
    depends_on:
      redis:
        condition: service_healthy
      chromadb:
        condition: service_started
    extra_hosts:
      - "host.docker.internal:host-gateway"
    restart: unless-stopped
    # Resource limits so chat-api does not starve the host (4-core machine)
    mem_limit: 4g
    cpus: '3'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # Nginx - Frontend & Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: rag-nginx
    ports:
      - "81:80"   # host:container — nginx listens on 80 inside container
    volumes:
      - ./frontend:/usr/share/nginx/html:ro
      - ./docker/nginx/nginx.conf:/etc/nginx/conf.d/default.conf:ro
    depends_on:
      chat-api:
        condition: service_started
    restart: unless-stopped

# ===========================================
# NETWORKS & VOLUMES (persist across compose down)
# ===========================================
networks:
  default:
    name: rag-network

# Named volumes: data survives `compose down`. Use `compose down -v` only to wipe data.
volumes:
  rag-redis-data:
